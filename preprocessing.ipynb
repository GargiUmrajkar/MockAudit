{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1209 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from transformers import GPT2TokenizerFast\n",
    "import json\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "# List of all PDF files with their part numbers in the name\n",
    "pdf_files_with_parts = {\n",
    "    'data/11.pdf': ['11'],\n",
    "    'data/200.pdf': ['200'],\n",
    "    'data/201.pdf': ['201'],\n",
    "    'data/202.pdf': ['202'],\n",
    "    'data/203.pdf': ['203'],\n",
    "    'data/205.pdf': ['205'],\n",
    "    'data/206.pdf': ['206'],\n",
    "    'data/207.pdf': ['207'],\n",
    "    'data/208.pdf': ['208'],\n",
    "    'data/209.pdf': ['209'],\n",
    "    'data/210.pdf': ['210'],\n",
    "    'data/211.pdf': ['211'],\n",
    "    'data/212.pdf': ['212'],\n",
    "    'data/216.pdf': ['216'],\n",
    "    'data/225.pdf': ['225'],\n",
    "    'data/226.pdf': ['226'],\n",
    "    'data/250.pdf': ['250'],\n",
    "    'data/251.pdf': ['251'],\n",
    "    'data/290.pdf': ['290'],\n",
    "    'data/299.pdf': ['299'],\n",
    "    'data/312.pdf': ['312'],\n",
    "    'data/314.pdf': ['314'],\n",
    "    'data/600.pdf': ['600'],\n",
    "    'data/601.pdf': ['601'],\n",
    "    'data/606.pdf': ['606'],\n",
    "    'data/607.pdf': ['607'],\n",
    "    'data/610.pdf': ['610'],\n",
    "    'data/630.pdf': ['630'],\n",
    "    'data/640.pdf': ['640'],\n",
    "    'data/660.pdf': ['660'],\n",
    "    'data/680.pdf': ['680'],\n",
    "    'data/820.pdf': ['820']\n",
    "}\n",
    "\n",
    "# Regex patterns to detect part titles, subparts, and sections\n",
    "part_pattern = re.compile(r\"PART\\s+(\\d+)\\s*—\\s*(.*)\", re.IGNORECASE)\n",
    "subpart_pattern = re.compile(r\"Subpart\\s+([A-Z])\\s*—\\s*(.*)\", re.IGNORECASE)\n",
    "section_pattern = re.compile(r\"^\\s*§\\s*\\d+\\.\\d+.*\", re.IGNORECASE | re.MULTILINE)\n",
    "\n",
    "# Function to extract the correct part and its title based on the part number\n",
    "def extract_correct_part_and_title(text, part_number):\n",
    "    part_text = []\n",
    "    part_title = None\n",
    "    lines = text.split(\"\\n\")\n",
    "    in_part = False\n",
    "\n",
    "    for line in lines:\n",
    "        match = part_pattern.match(line.strip())\n",
    "        if match:\n",
    "            current_part = match.group(1)\n",
    "            current_title = match.group(2)\n",
    "            if current_part == part_number:\n",
    "                part_title = current_title.strip()\n",
    "                i = lines.index(line) + 1\n",
    "                while i < len(lines) and not lines[i].strip().startswith(\"Subpart\") and not lines[i].strip().startswith(\"§\"):\n",
    "                    part_title += \" \" + lines[i].strip()\n",
    "                    i += 1\n",
    "                in_part = True\n",
    "            else:\n",
    "                if in_part:\n",
    "                    break\n",
    "        \n",
    "        if in_part:\n",
    "            part_text.append(line)\n",
    "    \n",
    "    if part_title and part_text:\n",
    "        return part_title, \"\\n\".join(part_text)\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "# Function to tokenize and chunk by token limit with overlap\n",
    "def chunk_text_by_token_limit(text, chunk_size=2000, overlap=100):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), chunk_size - overlap):\n",
    "        chunk_tokens = tokens[i:i + chunk_size]\n",
    "        chunk_text = tokenizer.decode(chunk_tokens)\n",
    "        chunks.append(chunk_text)\n",
    "    return chunks\n",
    "\n",
    "# Function to extract part, subpart, and section titles and chunk based on section headers\n",
    "def chunk_sections_with_titles(full_text, chunk_size=2000, overlap=100):\n",
    "    sections = {}\n",
    "    current_section = None\n",
    "    current_section_lines = []\n",
    "    current_part_title = None\n",
    "    current_subpart_title = None\n",
    "\n",
    "    lines = full_text.split(\"\\n\")\n",
    "\n",
    "    for idx, line in enumerate(lines):\n",
    "        part_match = part_pattern.match(line.strip())\n",
    "        if part_match:\n",
    "            current_part_title = extract_correct_part_and_title(full_text, part_match.group(1))[0]\n",
    "            continue\n",
    "\n",
    "        subpart_match = subpart_pattern.match(line.strip())\n",
    "        if subpart_match:\n",
    "            current_subpart_title = subpart_match.group(2).strip()\n",
    "            continue\n",
    "\n",
    "        section_match = section_pattern.match(line.strip())\n",
    "        if section_match:\n",
    "            if current_section:\n",
    "                section_text = \"\\n\".join(current_section_lines)\n",
    "                if len(tokenizer.encode(section_text)) > chunk_size:\n",
    "                    sections[current_section] = {\n",
    "                        \"chunks\": chunk_text_by_token_limit(section_text, chunk_size, overlap),\n",
    "                        \"part_title\": current_part_title,\n",
    "                        \"subpart_title\": current_subpart_title\n",
    "                    }\n",
    "                else:\n",
    "                    sections[current_section] = {\n",
    "                        \"chunks\": [section_text],\n",
    "                        \"part_title\": current_part_title,\n",
    "                        \"subpart_title\": current_subpart_title\n",
    "                    }\n",
    "\n",
    "            current_section = line.strip()\n",
    "            current_section_lines = [current_section]\n",
    "        else:\n",
    "            if current_section:\n",
    "                current_section_lines.append(line)\n",
    "\n",
    "    if current_section:\n",
    "        section_text = \"\\n\".join(current_section_lines)\n",
    "        if len(tokenizer.encode(section_text)) > chunk_size:\n",
    "            sections[current_section] = {\n",
    "                \"chunks\": chunk_text_by_token_limit(section_text, chunk_size, overlap),\n",
    "                \"part_title\": current_part_title,\n",
    "                \"subpart_title\": current_subpart_title\n",
    "            }\n",
    "        else:\n",
    "            sections[current_section] = {\n",
    "                \"chunks\": [section_text],\n",
    "                \"part_title\": current_part_title,\n",
    "                \"subpart_title\": current_subpart_title\n",
    "            }\n",
    "\n",
    "    return sections\n",
    "\n",
    "# Load all PDFs and extract the relevant parts, subparts, and sections with token limits\n",
    "all_extracted_parts = {}\n",
    "\n",
    "for file_path, part_numbers in pdf_files_with_parts.items():\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    pdf_documents = loader.load()\n",
    "    full_text = \"\\n\".join([doc.page_content for doc in pdf_documents])\n",
    "\n",
    "    for part_number in part_numbers:\n",
    "        part_title, correct_part_text = extract_correct_part_and_title(full_text, part_number)\n",
    "        if not part_title or not correct_part_text:\n",
    "            continue\n",
    "\n",
    "        chunked_sections = chunk_sections_with_titles(correct_part_text)\n",
    "        all_extracted_parts[file_path] = {\n",
    "            \"part_title\": part_title,\n",
    "            \"sections\": chunked_sections\n",
    "        }\n",
    "\n",
    "# Save document metadata (without embeddings yet)\n",
    "documents = []\n",
    "for file_path, part_info in all_extracted_parts.items():\n",
    "    part_title = part_info[\"part_title\"]\n",
    "    for section, section_info in part_info[\"sections\"].items():\n",
    "        subpart_title = section_info[\"subpart_title\"]\n",
    "        for chunk in section_info[\"chunks\"]:\n",
    "            doc = {\n",
    "                \"page_content\": chunk,\n",
    "                \"metadata\": {\n",
    "                    \"file_path\": file_path,\n",
    "                    \"part_title\": part_title,\n",
    "                    \"subpart_title\": subpart_title,\n",
    "                    \"section\": section\n",
    "                }\n",
    "            }\n",
    "            documents.append(doc)\n",
    "\n",
    "# Save the metadata for all documents\n",
    "with open('document_metadata.json', 'w') as f:\n",
    "    json.dump(documents, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bd/5xkghs212xj6lnp1k9q9w48h0000gn/T/ipykernel_93662/310713930.py:7: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embedding_model = HuggingFaceEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import faiss\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Initialize HuggingFace embedding model\n",
    "embedding_model = HuggingFaceEmbeddings()\n",
    "\n",
    "# Load the document metadata (without embeddings)\n",
    "with open('document_metadata.json', 'r') as f:\n",
    "    documents = json.load(f)\n",
    "\n",
    "# Generate embeddings for each document chunk\n",
    "for doc in documents:\n",
    "    embedding = embedding_model.embed_query(doc['page_content'])\n",
    "    doc['metadata']['embedding'] = embedding\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = len(documents[0]['metadata']['embedding'])\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Add embeddings to FAISS index\n",
    "embeddings = [doc['metadata']['embedding'] for doc in documents]\n",
    "embedding_matrix = np.array(embeddings).astype('float32')\n",
    "index.add(embedding_matrix)\n",
    "\n",
    "# Save FAISS index to disk\n",
    "faiss.write_index(index, \"faiss_index.index\")\n",
    "\n",
    "# Save updated metadata with embeddings\n",
    "with open('document_metadata_with_embeddings.json', 'w') as f:\n",
    "    json.dump(documents, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mockauditvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
